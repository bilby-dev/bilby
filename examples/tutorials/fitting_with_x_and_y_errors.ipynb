{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a model to data with both x and y errors with `Bilby`\n",
    "\n",
    "Usually when we fit a model to data with a Gaussian Likelihood we assume that we know x values exactly. This is almost never the case. Here we show how to fit a model with errors in both x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilby\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bilby.core.utils.random import seed, rng\n",
    "\n",
    "#sets seed of bilby's generator \"rng\" to \"123\"\n",
    "seed(123)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data\n",
    "\n",
    "First we create the data and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model, a line\n",
    "def model(x, m, c, **kwargs):\n",
    "    y = m * x + c\n",
    "    return y\n",
    "\n",
    "\n",
    "# make a function to create and plot our data\n",
    "def make_data(points, m, c, xerr, yerr, seed):\n",
    "    xtrue = np.linspace(0, 100, points)\n",
    "    ytrue = model(x=xtrue, m=m, c=c)\n",
    "\n",
    "    xerr_vals = xerr * rng.standard_normal(points)\n",
    "    yerr_vals = yerr * rng.standard_normal(points)\n",
    "    xobs = xtrue + xerr_vals\n",
    "    yobs = ytrue + yerr_vals\n",
    "\n",
    "    plt.errorbar(xobs, yobs, xerr=xerr, yerr=yerr, fmt=\"x\")\n",
    "    plt.errorbar(xtrue, ytrue, yerr=yerr, color=\"black\", alpha=0.5)\n",
    "    plt.xlim(0, 100)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    data = {\n",
    "        \"xtrue\": xtrue,\n",
    "        \"ytrue\": ytrue,\n",
    "        \"xobs\": xobs,\n",
    "        \"yobs\": yobs,\n",
    "        \"xerr\": xerr,\n",
    "        \"yerr\": yerr,\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data = make_data(points=30, m=5, c=10, xerr=5, yerr=5, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our prior and sampler settings\n",
    "\n",
    "Now lets set up the prior and bilby output directory/sampler settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up bilby priors\n",
    "priors = dict(\n",
    "    m=bilby.core.prior.Uniform(0, 30, \"m\"), c=bilby.core.prior.Uniform(0, 30, \"c\")\n",
    ")\n",
    "\n",
    "sampler_kwargs = dict(priors=priors, sampler=\"bilby_mcmc\", nsamples=1000, printdt=5, outdir=\"outdir\", verbose=False, clean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit with exactly known x-values\n",
    "\n",
    "Our first step is to recover the straight line using a simple Gaussian Likelihood that only takes into account the y errors. Under the assumption we know x exactly. In this case, we pass in xtrue for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_x = bilby.core.likelihood.GaussianLikelihood(\n",
    "    x=data[\"xtrue\"], y=data[\"yobs\"], func=model, sigma=data[\"yerr\"]\n",
    ")\n",
    "result_known_x = bilby.run_sampler(\n",
    "    likelihood=known_x,\n",
    "    label=\"known_x\",\n",
    "    **sampler_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = result_known_x.plot_corner(truth=dict(m=5, c=10), titles=True, save=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit with unmodeled uncertainty in the x-values\n",
    "\n",
    "As expected this is easy to recover and the sampler does a good job. However this was made too easy - by passing in the 'true' values of x. Lets see what happens when we pass in the observed values of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_x = bilby.core.likelihood.GaussianLikelihood(\n",
    "    x=data[\"xobs\"], y=data[\"yobs\"], func=model, sigma=data[\"yerr\"]\n",
    ")\n",
    "result_incorrect_x = bilby.run_sampler(\n",
    "    likelihood=incorrect_x,\n",
    "    label=\"incorrect_x\",\n",
    "    **sampler_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = result_incorrect_x.plot_corner(truth=dict(m=5, c=10), titles=True, save=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit with modeled uncertainty in x-values\n",
    "\n",
    "This is not good as there is unmodelled uncertainty in our `x` values.\n",
    "Getting around this requires marginalisation of the true x values or sampling over them. \n",
    "See discussion in section 7 of https://arxiv.org/pdf/1008.4686.pdf.\n",
    "\n",
    "For this, we will have to define a new likelihood class.\n",
    "By subclassing the base `bilby.core.likelihood.Likelihood` class we can do this fairly simply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianLikelihoodUncertainX(bilby.core.likelihood.Likelihood):\n",
    "    def __init__(self, xobs, yobs, xerr, yerr, function):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xobs, yobs: array_like\n",
    "            The data to analyse\n",
    "        xerr, yerr: array_like\n",
    "            The standard deviation of the noise\n",
    "        function:\n",
    "            The python function to fit to the data\n",
    "        \"\"\"\n",
    "        super(GaussianLikelihoodUncertainX, self).__init__(dict())\n",
    "        self.xobs = xobs\n",
    "        self.yobs = yobs\n",
    "        self.yerr = yerr\n",
    "        self.xerr = xerr\n",
    "        self.function = function\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        variance = (self.xerr * self.parameters[\"m\"]) ** 2 + self.yerr**2\n",
    "        model_y = self.function(self.xobs, **self.parameters)\n",
    "        residual = self.yobs - model_y\n",
    "\n",
    "        ll = -0.5 * np.sum(residual**2 / variance + np.log(variance))\n",
    "\n",
    "        return -0.5 * np.sum(residual**2 / variance + np.log(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_unknown_x = GaussianLikelihoodUncertainX(\n",
    "    xobs=data[\"xobs\"],\n",
    "    yobs=data[\"yobs\"],\n",
    "    xerr=data[\"xerr\"],\n",
    "    yerr=data[\"yerr\"],\n",
    "    function=model,\n",
    ")\n",
    "result_unknown_x = bilby.run_sampler(\n",
    "    likelihood=gaussian_unknown_x,\n",
    "    label=\"unknown_x\",\n",
    "    **sampler_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = result_unknown_x.plot_corner(truth=dict(m=5, c=10), titles=True, save=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The inferred posterior is consistent with the true values."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
